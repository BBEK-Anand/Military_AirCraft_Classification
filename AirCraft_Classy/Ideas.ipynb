{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17e418e-3b25-4be9-872b-9ed7ce75a9c7",
   "metadata": {},
   "source": [
    "Great! If you're looking to explore more advanced architectures like **Transformers** and **Attention mechanisms** in combination with a pre-trained model, you can definitely leverage them to enhance performance. Transformer-based models have shown remarkable success in vision tasks, especially with **Vision Transformers (ViT)** and hybrid models like **DeiT** (Data-efficient Transformers). These models can be especially useful for tasks with highly specialized images like military aircraft classification.\n",
    "\n",
    "Below are some advanced ideas, including the use of Transformers, Attention mechanisms, and other cutting-edge techniques, which might be helpful for your classification task:\n",
    "\n",
    "### 1. **Vision Transformer (ViT)**\n",
    "\n",
    "**Vision Transformers (ViT)** have shown to outperform CNNs in many computer vision tasks, especially when there is a large amount of data. ViT divides images into patches and applies transformer-based attention mechanisms to learn the relationships between the patches.\n",
    "\n",
    "If you're working with high-quality data and need to capture fine-grained patterns, a Vision Transformer might be a great fit for this task.\n",
    "\n",
    "#### How to use ViT with transfer learning:\n",
    "- You can use a pre-trained ViT model from the **Hugging Face** library or the **timm** library (which includes a variety of pre-trained models).\n",
    "\n",
    "```python\n",
    "from timm import create_model\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load pre-trained ViT model\n",
    "model = create_model('vit_base_patch16_224', pretrained=True)  # Using base ViT model with 16x16 patch size\n",
    "\n",
    "# Modify the classifier for your number of classes (74 classes)\n",
    "model.head = nn.Linear(model.head.in_features, 74)  # Change output layer to match 74 classes\n",
    "\n",
    "# Set the model to the device (CUDA or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define a simple transform for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # Resize to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\n",
    "])\n",
    "\n",
    "# Example Training Loop\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:  # Assuming you have a train DataLoader\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "### 2. **Hybrid Models: CNN + Transformer**\n",
    "\n",
    "Another interesting approach is to combine **CNNs** with **Transformers** (hybrid models). This architecture leverages the best of both worlds: the powerful feature extraction ability of CNNs and the global attention mechanism of Transformers. One such hybrid architecture is **Convolutional Vision Transformers (CvT)**, which applies convolutions before applying the transformer layers.\n",
    "\n",
    "- **CvT** models (like **CvT-21** or **CvT-13**) are available in the **timm** library and could work well for your task.\n",
    "\n",
    "Here’s a simplified outline of how to use a hybrid model:\n",
    "\n",
    "```python\n",
    "from timm import create_model\n",
    "\n",
    "# Load pre-trained CvT model (a hybrid CNN + Transformer)\n",
    "model = create_model('cvt_21', pretrained=True)\n",
    "\n",
    "# Modify the classifier for your dataset\n",
    "model.head = nn.Linear(model.head.in_features, 74)  # 74 classes\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "### 3. **Attention Mechanisms (Attention-based CNNs)**\n",
    "\n",
    "If you're looking for **Attention Mechanisms** but want to stick with a convolutional backbone, you can introduce **Attention layers** into the network. **Squeeze-and-Excitation Networks (SE-Nets)**, **CBAM (Convolutional Block Attention Module)**, or **Non-Local Networks** are commonly used for this.\n",
    "\n",
    "These attention modules help the model focus on the most relevant parts of the image and can significantly improve performance, especially for tasks with complex images like aircraft classification.\n",
    "\n",
    "#### Squeeze-and-Excitation (SE) Block:\n",
    "- The SE block adjusts the feature maps based on channel-wise attention and helps the model focus on the most important channels.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_pool = torch.mean(x, dim=[2, 3], keepdim=True)  # Global average pooling\n",
    "        avg_pool = avg_pool.view(b, c)\n",
    "        out = self.fc1(avg_pool)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out).view(b, c, 1, 1)\n",
    "        return x * out  # Scale the feature maps by the attention map\n",
    "\n",
    "# Example of adding an SE block after a CNN layer\n",
    "class SEConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=74):\n",
    "        super(SEConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.se1 = SEBlock(64)  # Attention mechanism after conv1\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(64 * 112 * 112, num_classes)  # Adjust size based on input dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.se1(self.conv1(x)))  # Apply SE after conv1\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "### 4. **DeiT (Data-Efficient Transformer)**\n",
    "\n",
    "DeiT (Data-Efficient Transformer) is an efficient version of ViT. It’s designed to perform well on smaller datasets by leveraging **distillation** (using a teacher model to guide the training of the student model). This can be particularly helpful if your dataset is not very large but you still want to take advantage of transformer-based architecture.\n",
    "\n",
    "```python\n",
    "from timm import create_model\n",
    "\n",
    "# Load pre-trained DeiT model (Data-efficient Transformer)\n",
    "model = create_model('deit_base_distilled_patch16_224', pretrained=True)\n",
    "\n",
    "# Modify the classifier for your dataset\n",
    "model.head = nn.Linear(model.head.in_features, 74)  # 74 classes\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "### 5. **Attention-based Transfer Learning**\n",
    "\n",
    "You can also implement **self-attention layers** on top of your CNN backbone. A simple way to add attention without changing the backbone is to integrate **self-attention** layers into your network, where the model learns to focus on specific regions in the image.\n",
    "\n",
    "- Use **Self-Attention** (implemented in many transformer models) as an additional mechanism to focus on important areas of the image. This is useful in scenarios like object classification, where the most relevant features may appear in different regions of the image (such as the aircraft's wings, tail, etc.).\n",
    "\n",
    "#### Self-Attention Layer:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))  # Attention coefficient\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        attention_map = torch.matmul(query.view(query.size(0), query.size(1), -1),\n",
    "                                     key.view(key.size(0), key.size(1), -1).transpose(1, 2))\n",
    "        attention_map = torch.softmax(attention_map, dim=-1)\n",
    "        \n",
    "        out = torch.matmul(attention_map, value.view(value.size(0), value.size(1), -1).transpose(1, 2))\n",
    "        out = out.view(x.size(0), x\n",
    "\n",
    ".size(1), x.size(2), x.size(3))\n",
    "        return self.gamma * out + x  # Apply attention\n",
    "\n",
    "# Example of adding self-attention after a CNN layer\n",
    "class CNNWithAttention(nn.Module):\n",
    "    def __init__(self, num_classes=74):\n",
    "        super(CNNWithAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.attn1 = SelfAttention(64)  # Self-attention after conv1\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(64 * 112 * 112, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.attn1(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Using **Transformers** (ViT, DeiT), **hybrid CNN-Transformer models**, or adding **attention mechanisms** like **SE-Net** and **self-attention layers** to your existing CNN can be a powerful strategy to improve the model’s performance on your military aircraft classification task.\n",
    "\n",
    "By leveraging pre-trained models, you're essentially utilizing **transfer learning**, which can help the model generalize better, especially with smaller datasets or complex tasks.\n",
    "\n",
    "If you need help setting up any of these architectures, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a95fff-43af-482e-aa93-20cf0f65e3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
